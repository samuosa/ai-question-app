topic;question;a;b;c;d;correctAnswer;explanation;difficulty;language
##################
###### EVAL ######
##################
Data Processing & Storage;Cymbal Direct drones continuously send data during deliveries. You need to process and analyze the incoming telemetry data. After processing, the data should be retained, but it will only be accessed once every month or two. Your CIO has issued a directive to incorporate managed services wherever possible. You want a cost-effective solution to process the incoming streams of data.;Ingest data with ClearBlade IoT Core, process it with Dataprep, and store it in a Coldline Cloud Storage bucket.;Ingest data with ClearBlade IoT Core, and then publish to Pub/Sub. Use BigQuery to process the data, and store it in a Standard Cloud Storage bucket.;Ingest data with ClearBlade IoT Core, and then publish to Pub/Sub. Use Dataflow to process the data, and store it in a Nearline Cloud Storage bucket.;Ingest data with ClearBlade IoT Core, and then store it in BigQuery.;c;Dataflow is a managed service for stream and batch processing, and Nearline storage is optimal for monthly access frequency.;medium;EN
Networking & VPC Design;You are creating a new project. You plan to set up a Dedicated interconnect between two of your data centers in the near future and want to ensure that your resources are only deployed to the same regions where your data centers are located. You need to make sure that you don’t have any overlapping IP addresses that could cause conflicts when you set up the interconnect. You want to use RFC 1918 class B address space. What should you do?;Create a new project, delete the default VPC network, set up an auto mode VPC network, and then use the default 10.x.x.x network range to create subnets in your desired regions.;Create a new project, delete the default VPC network, set up a custom mode VPC network, and then use IP addresses in the 172.16.x.x address range to create subnets in your desired regions.;Create a new project, leave the default network in place, and then use the default 10.x.x.x network range to create subnets in your desired regions.;Create a new project, delete the default VPC network, set up the network in custom mode, and then use IP addresses in the 192.168.x.x address range to create subnets in your desired zones. Use VPC Network Peering to connect the zones in the same region to create regional networks.;b;Custom mode networks provide control over IP ranges, avoiding conflicts and enabling proper interconnect setup.;medium;EN
Networking & Connectivity;Cymbal Direct is working with Cymbal Retail, a separate, autonomous division of Cymbal with different staff, networking teams, and data center. Cymbal Direct and Cymbal Retail are not in the same Google Cloud organization. Cymbal Retail needs access to Cymbal Direct’s web application for making bulk orders, but the application will not be available on the public internet. You want to ensure that Cymbal Retail has access to your application with low latency. You also want to avoid egress network charges if possible. What should you do?;If Cymbal Retail does not have access to a Google Cloud data center, use Carrier Peering to connect the two networks.;Verify that the subnet range Cymbal Retail is using doesn’t overlap with Cymbal Direct’s subnet range, and then enable VPC Network Peering for the project.;Verify that the subnet Cymbal Retail is using has the same IP address range with Cymbal Direct’s subnet range, and then enable VPC Network Peering for the project.;Specify Cymbal Direct’s project as the Shared VPC host project, and then configure Cymbal Retail’s project as a service project.;b;VPC Network Peering enables private connectivity between projects and organizations with low latency and no egress charges.;medium;EN
Kubernetes & Stateful Applications;You are working with a client who is using Google Kubernetes Engine (GKE) to migrate applications from a virtual machine–based environment to a microservices-based architecture. Your client has a complex legacy application that stores a significant amount of data on the file system of its VM. You do not want to re-write the application to use an external service to store the file system data. What should you do?;In Cloud Shell, create a YAML file defining your Container called build.yaml. Create a Container in GKE by running the command gcloud builds submit –config build.yaml .;In Cloud Shell, create a YAML file defining your Pod called pod.yaml. Create a Pod in GKE by running the command kubectl apply -f pod.yaml;In Cloud Shell, create a YAML file defining your StatefulSet called statefulset.yaml. Create a StatefulSet in GKE by running the command kubectl apply -f statefulset.yaml;In Cloud Shell, create a YAML file defining your Deployment called deployment.yaml. Create a Deployment in GKE by running the command kubectl apply -f deployment.yaml;c;A StatefulSet manages persistent Pods with storage defined by PersistentVolumeClaims, preserving state across restarts.;medium;EN
VM Migration;You are working in a mixed environment of VMs and Kubernetes. Some of your resources are on-premises, and some are in Google Cloud. Using containers as a part of your CI/CD pipeline has sped up releases significantly. You want to start migrating some of those VMs to containers so you can get similar benefits. You want to automate the migration process where possible. What should you do?;Manually create a GKE cluster. Use Cloud Build to import VMs and convert them to containers.;Use Migrate for Anthos to automate the creation of Compute Engine instances to import VMs and convert them to containers.;Use Migrate for Compute Engine to import VMs and convert them to containers.;Manually create a GKE cluster, and then use Migrate for Anthos to set up the cluster, import VMs, and convert them to containers.;d;Migrate for Anthos automates conversion of VMs to containers running in GKE, simplifying migration and modernization.;medium;EN
Cloud Architecture & Scaling;Cymbal Direct has created a proof of concept for a social integration service that highlights images of its products from social media. The proof of concept is a monolithic application running on a single SuSE Linux virtual machine (VM). The current version requires increasing the VM’s CPU and RAM in order to scale. You would like to refactor the VM so that you can scale out instead of scaling up. What should you do?;Make sure that the application declares any dependent requirements in a requirements.txt or equivalent statement so that they can be referenced in a startup script, and attach external persistent volumes to the VMs.;Make sure that the application declares any dependent requirements in a requirements.txt or equivalent statement so that they can be referenced in a startup script. Specify the startup script in a managed instance group template, and use an autoscaling policy.;Use containers instead of VMs, and use a GKE autoscaling deployment.;Move the existing codebase and VM provisioning scripts to git, and attach external persistent volumes to the VMs.;b;Using managed instance groups with autoscaling enables horizontal scaling while keeping configuration declarative and repeatable.;medium;EN
SRE & Monitoring;Customers need to have a good experience when accessing your web application so they will continue to use your service. You want to define key performance indicators (KPIs) to establish a service level objective (SLO). Which KPI could you use?;Eighty-five percent of requests succeed when aggregated over 1 minute;Eighty-five percent of requests are successful;Eighty-five percent of customers are satisfied users;Low latency for > 85% of requests when aggregated over 1 minute;a;This KPI is specific and measurable: it defines a success rate and an aggregation window, allowing comparison against an SLO threshold.;easy;EN
Database Selection;Cymbal Direct is evaluating database options to store the analytics data from its experimental drone deliveries. You're currently using a small cluster of MongoDB NoSQL database servers. You want to move to a managed NoSQL database service with consistent low latency that can scale throughput seamlessly and can handle the petabytes of data you expect after expanding to additional markets. What should you do?;Extract the data from MongoDB. Insert the data into Firestore using Native mode.;Extract the data from MongoDB, and insert the data into BigQuery.;Create a Bigtable instance, extract the data from MongoDB, and insert the data into Bigtable.;Extract the data from MongoDB. Insert the data into Firestore using Datastore mode.;c;Bigtable offers petabyte-scale performance with sub-10ms latency, ideal for analytical and IoT workloads.;medium;EN
Compute Engine Optimization;Cymbal Direct developers have written a new application. Based on initial usage estimates, you decide to run the application on Compute Engine instances with 15 Gb of RAM and 4 CPUs. These instances store persistent data locally. After the application runs for several months, historical data indicates that the application requires 30 Gb of RAM. Cymbal Direct management wants you to make adjustments that will minimize costs. What should you do?;Stop the instance, and then use the command gcloud compute instances set-machine-type VM_NAME --machine-type 2-custom-4-30720. Start the instance again.;Stop the instance, and then use the command gcloud compute instances set-machine-type VM_NAME --machine-type 2-custom-4-30720. Set the instance’s metadata to: preemptible: true. Start the instance again.;Stop the instance, and then use the command gcloud compute instances set-machine-type VM_NAME --machine-type e2-standard-8. Set the instance’s metadata to: preemptible: true. Start the instance again.;Stop the instance, and then use the command gcloud compute instances set-machine-type VM_NAME --machine-type e2-standard-8. Start the instance again.;a;Custom machine types allow adjusting CPU and memory precisely, reducing cost compared to fixed instance types.;easy;EN
Network Connectivity;Cymbal Direct's employees will use Google Workspace. Your current on-premises network cannot meet the requirements to connect to Google's public infrastructure. What should you do?;Connect the network to a Google point of presence, and enable Direct Peering.;Order a Dedicated Interconnect from a Google Cloud partner, and ensure that proper routes are configured.;Connect the on-premises network to Google’s public infrastructure via a partner that supports Carrier Peering.;Order a Partner Interconnect from a Google Cloud partner, and ensure that proper routes are configured.;c;Carrier Peering (via a partner ISP) provides access to Google public services like Workspace when you can’t meet Direct Peering requirements; Dedicated/Partner Interconnect are for private VPC connectivity, not needed here.;medium;EN

Security & Compliance;Cymbal Direct must meet compliance requirements. You need to ensure that employees with valid accounts cannot access their VPC network from locations outside of its secure corporate network, including from home. You also want a high degree of visibility into network traffic for auditing and forensics purposes. What should you do?;Enable Identity-Aware Proxy (IAP) to allow users to access services securely. Use Google Cloud Observability to view audit logs for the networks you need to monitor.;Enable VPC Service Controls, and use Google Cloud Observability to view audit logs for the networks you need to monitor.;Enable VPC Service Controls, define a network perimeter to restrict access to authorized networks, and enable VPC Flow Logs for the networks you need to monitor.;Ensure that all users install Cloud VPN. Enable VPC Flow Logs for the networks you need to monitor.;c;VPC Service Controls define a network perimeter to restrict access to approved IP ranges, and VPC Flow Logs give visibility for auditing and forensics.;medium;EN
API Design;Cymbal Direct wants to allow partners to make orders programmatically, without having to speak on the phone with an agent. What should you consider when designing the API?;The API backend should be loosely coupled. Clients should not be required to know too many details of the services they use. REST APIs using gRPC should be used for all external APIs.;The API backend should be loosely coupled. Clients should not be required to know too many details of the services they use. For REST APIs, HTTP(S) is the most common protocol.;The API backend should be tightly coupled. Clients should know a significant amount about the services they use. REST APIs using gRPC should be used for all external APIs.;The API backend should be tightly coupled. Clients should know a significant amount about the services they use. For REST APIs, HTTP(S) is the most common protocol used.;b;External APIs should use REST over HTTP(S) and maintain loose coupling for scalability and maintainability.;easy;EN
Infrastructure as Code;Cymbal Direct needs to use a tool to deploy its infrastructure. You want something that allows for repeatable deployment processes, uses a declarative language, and allows parallel deployment. You also want to deploy infrastructure as code on Google Cloud and other cloud providers. What should you do?;Develop in Docker containers for portability and ease of deployment.;Use Google Kubernetes Engine (GKE) to create deployments and manifests for your applications.;Automate the deployment with Cloud Deployment Manager.;Automate the deployment with Terraform scripts.;d;Terraform provides declarative, multi-cloud infrastructure as code with parallel deployment and repeatable workflows.;medium;EN
Kubernetes Scaling;You have deployed your frontend web application in Kubernetes. Based on historical use, you need three pods to handle normal demand. Occasionally your load will roughly double. A load balancer is already in place. How could you configure your environment to efficiently meet that demand?;Use the "kubectl autoscale" command to change the deployment’s maximum number of instances to six.;Edit your deployment's configuration file and change the number of replicas to six.;Use the "kubectl autoscale" command to change the pod's maximum number of instances to six.;Edit your pod's configuration file and change the number of replicas to six.;a;Horizontal Pod Autoscaler on a Deployment scales replicas automatically based on observed load, allowing up to six pods as needed.;easy;EN
Data Retention & Privacy;Cymbal Direct's user account management app allows users to delete their accounts whenever they like. Cymbal Direct also has a very generous 60-day return policy for users. The customer service team wants to make sure that they can still refund or replace items for a customer even if the customer’s account has been deleted. What can you do to ensure that the customer service team has access to relevant account information?;Restore a previous copy of the user information database from a snapshot. Have a database administrator capture needed information about the customer.;Disable the account. Export account information to Cloud Storage. Have the customer service team permanently delete the data after 30 days.;Ensure that the user clearly understands that after they delete their account, all their information will also be deleted. Remind them to download a copy of their order history and account information before deleting their account. Have the support agent copy any open or recent orders to a shared spreadsheet.;Temporarily disable the account for 30 days. Export account information to Cloud Storage, and enable lifecycle management to delete the data in 60 days.;d;Temporarily disabling and exporting user data with lifecycle-managed deletion satisfies both compliance and refund needs.;medium;EN
CI/CD Pipelines;Cymbal Direct wants to create a pipeline to automate the building of new application releases. What sequence of steps should you use?;Set up a source code repository. Check in code. Run unit tests. Build a Docker container. Deploy.;Run unit tests. Deploy. Build a Docker container. Check in code. Set up a source code repository.;Check in code. Set up a source code repository. Run unit tests. Deploy. Build a Docker container.;Set up a source code repository. Run unit tests. Check in code. Deploy. Build a Docker container.;a;A standard CI/CD workflow starts with source control, followed by testing, container build, and deployment.;easy;EN
VM Migration;Your existing application runs on Ubuntu Linux VMs in an on-premises hypervisor. You want to deploy the application to Google Cloud with minimal refactoring. What should you do?;Write Terraform scripts to deploy the application as Compute Engine instances.;Use a Dedicated or Partner Interconnect to connect the on-premises network where your application is running to your VPC: Configure an endpoint for a global external Application Load Balancer that connects to the existing VMs.;Isolate the core features that the application provides. Use App Engine to deploy each feature independently as a microservice.;Set up a Google Kubernetes Engine (GKE) cluster, and then create a deployment with an autoscaler.;a;Deploying the app as Compute Engine instances requires minimal changes and maintains compatibility with existing VM workloads.;medium;EN
Application Deployment Architecture;You are working with a client who has built a secure messaging application. The application is open source and consists of two components. The first component is a web app, written in Go, which is used to register an account and authorize the user’s IP address. The second is an encrypted chat protocol that uses TCP to talk to the backend chat servers running Debian. If the client's IP address doesn't match the registered IP address, the application is designed to terminate their session. The number of clients using the service varies greatly based on time of day, and the client wants to be able to easily scale as needed. What should you do?;Deploy the web application using the App Engine flexible environment with a global external Application Load Balancer and a network endpoint group. Use an unmanaged instance group for the backend chat servers. Use an external passthrough Network Load Balancer to load-balance traffic across the backend chat servers.;Deploy the web application using the App Engine standard environment with a global external Application Load Balancer and a network endpoint group. Use an unmanaged instance group for the backend chat servers. Use an external network load balancer to load-balance traffic across the backend chat servers.;Deploy the web application using the App Engine standard environment with a global external Application Load Balancer and a network endpoint group. Use a managed instance group for the backend chat servers. Use an external passthrough Network Load Balancer to load-balance traffic across the backend chat servers.;Deploy the web application using the App Engine standard environment with a global external Application Load Balancer and a network endpoint group. Use a managed instance group for the backend chat servers. Use a global external Network Load Balancer with SSL proxy to load-balance traffic across the backend chat servers.;c;App Engine standard scales automatically for the web app, while a managed instance group behind a passthrough NLB efficiently handles TCP chat traffic.;hard;EN
Compute Engine Security;Cymbal Direct wants a layered approach to security when setting up Compute Engine instances. What are some options you could use to make your Compute Engine instances more secure?;Use network tags to allow traffic only from certain sources and ports. Use a Compute Engine service account.;Use labels to allow traffic only from certain sources and ports. Use a Compute Engine service account.;Use network tags to allow traffic only from certain sources and ports. Turn on Secure boot and vTPM.;Use labels to allow traffic only from certain sources and ports. Turn on Secure boot and vTPM.;c;Network tags control firewall policies, while Secure Boot and vTPM protect the integrity of the instance’s operating environment.;medium;EN
Load Balancing & Traffic Routing;You need to deploy a load balancer for a web-based application with multiple backends in different regions. You want to direct traffic to the backend closest to the end user, but also to different backends based on the URL the user is accessing. Which of the following could be used to implement this?;The request is received by the global external Application Load Balancer. A global forwarding rule sends the request to a target proxy, which checks the URL map and selects the backend service. The backend service sends the request to Compute Engine instance groups in multiple regions.;The request is matched by a URL map and then sent to a proxy Network Load Balancer. A global forwarding rule sends the request to a target proxy, which selects a backend service and sends the request to Compute Engine instance groups in multiple regions.;The request is received by the proxy Network Load Balancer, which uses a global forwarding rule to check the URL map, then sends the request to a backend service. The request is processed by Compute Engine instance groups in multiple regions.;The request is matched by a URL map and then sent to a global external Application Load Balancer. A global forwarding rule sends the request to a target proxy, which selects a backend service. The backend service sends the request to Compute Engine instance groups in multiple regions.;a;Global external HTTP(S) Load Balancer supports both geographic routing and URL-based routing via target proxies and URL maps.;hard;EN

API Design & Strategy;Michael is the owner/operator of “Zneeks,” a retail shoe store that caters to sneaker aficionados. He regularly works with customers who order small batches of custom shoes. Michael is interested in using Cymbal Direct to manufacture and ship custom batches of shoes to these customers. Reasonably tech-savvy but not a developer, Michael likes using Cymbal Direct's partner purchase portal but wants the process to be easy. What is an example of a user story that could describe Michael’s persona?;Zneeks is a retail shoe store that caters to sneaker aficionados.;As a shoe retailer, Michael wants to send Cymbal Direct custom purchase orders so that batches of custom shoes are sent to his customers.;Michael is a tech-savvy owner/operator of a small business.;Michael is reasonably tech-savvy but needs Cymbal Direct's partner purchase portal to be easy;b;A standard user story follows the specific format 'As a [role], I want [action] so that [benefit],' which Option B correctly demonstrates.;easy;EN
Cloud Architecture;The number of requests received by your application is nearing the maximum specified in your design. You want to limit the number of incoming requests until the system can handle the workload. What design pattern does this situation describe?;Applying exponential backoff;Applying graceful degradation;Increasing jitter;Applying a circuit breaker;d;According to Google Cloud Architecture Framework reliability patterns, the Circuit Breaker pattern prevents an application from performing an operation that is likely to fail (such as when overloaded) by failing fast and rejecting requests, which limits the load and gives the system time to recover.;medium;EN
Cloud Build;You have implemented a manual CI/CD process for the container services required for the next implementation of the Cymbal Direct’s Drone Delivery project. You want to automate the process. What should you do?;Implement a build trigger that applies your build configuration when a new software update is committed to Cloud Source Repositories.;Specify the name of your Container Registry in your Cloud Build configuration.;Configure and push a manifest file into an environment repository in Cloud Source Repositories.;Implement and reference a source repository in your Cloud Build configuration file.;a;According to Cloud Build documentation, automating a CI/CD pipeline requires creating a build trigger. This trigger watches for source code changes (like commits to Cloud Source Repositories) and automatically executes the build configuration, replacing the manual process.;easy;EN
Disaster Recovery;You are implementing a disaster recovery plan for the cloud version of your drone solution. Sending videos to the pilots is crucial from an operational perspective. What design pattern should you choose for this part of your architecture?;Hot with a low recovery time objective (RTO);Cold with a low recovery time objective (RTO);Hot with a high recovery time objective (RTO);Warm with a high recovery time objective (RTO);a;Because the video feed is operationally crucial, the system requires minimal downtime, which translates to a Low Recovery Time Objective (RTO). According to GCP Disaster Recovery guides, a 'Hot' pattern is required to achieve low RTO, as the recovery site is fully functioning and ready to take traffic immediately.;medium;EN
DevOps;Cymbal Direct wants to improve its drone pilot interface. You want to collect feedback on proposed changes from the community of pilots before rolling out updates systemwide. What type of deployment pattern should you implement?;You should implement an in-place release.;You should implement canary testing.;You should implement a blue/green deployment.;You should implement A/B testing.;d;According to Google Cloud's application deployment patterns, A/B testing is the strategy used to evaluate the effectiveness of a new feature or interface by showing it to a subset of users to gather feedback or compare metrics against the existing version, whereas Canary is primarily used to minimize risk/errors.;medium;EN
Compute Engine;The pilot subsystem in your Delivery by Drone service is critical to your service. You want to ensure that connections to the pilots can survive a VM outage without affecting connectivity. What should you do?;Deploy a load balancer to distribute traffic across multiple machines.;Implement a managed instance group and load balancer.;Create persistent disk snapshots.;Configure proper startup scripts for your VMs.;b;According to GCP High Availability best practices, combining a Managed Instance Group (MIG) with a Load Balancer is the correct architecture; the MIG provides auto-healing to replace failed VMs automatically, while the load balancer routes traffic only to healthy instances, ensuring uninterrupted connectivity.;medium;EN
Terraform;You are asked to implement a lift and shift operation for Cymbal Direct’s Social Media Highlighting service. You compose a Terraform configuration file to build all the necessary Google Cloud resources. What is the next step in the Terraform workflow for this effort?;Run terraform apply to deploy the resources described in the configuration file.;Run terraform plan to verify the contents of the Terraform configuration file.;Run terraform init to download the necessary provider modules.;Commit the configuration file to your software repository;c;According to the standard Terraform workflow, after writing a configuration file, you must run 'terraform init' to initialize the working directory and download the necessary provider plugins before you can run 'plan' or 'apply'.;easy;EN
DevOps;You want to establish procedures for testing the resilience of the delivery-by-drone solution. How would you simulate a scalability issue?;Inject a bad health check for one or more of your resources.;Block access to all resources in a zone.;Load test your application to see how it responds.;Block access to storage assets in one of your zones.;c;According to Google Cloud's reliability and SRE best practices, simulating a scalability issue requires generating high volumes of traffic (load testing) to verify if the application successfully auto-scales resources up and down to meet demand without degrading performance.;easy;EN
DevOps;Your development team used Cloud Source Repositories, Cloud Build, and Artifact Registry to successfully implement the build portion of an application's CI/CD process.. However, the deployment process is erroring out. Initial troubleshooting shows that the runtime environment does not have access to the build images. You need to advise the team on how to resolve the issue. What could cause this problem?;You need to specify the Artifact Registry image by name.;The runtime environment does not have permissions to Cloud Source Repositories in your current project.;The Artifact Registry might be in a different project.;The runtime environment does not have permissions to the Artifact Registry in your current project.;c;According to GCP IAM defaults, runtime environments (like Compute Engine default service accounts) automatically have read access to Artifact Registry repositories within the *same* project. Therefore, if access is denied, the most likely cause is that the Artifact Registry resides in a *different* project, where explicit cross-project IAM permissions are required but missing.;medium;EN
Compute Engine;You have an application implemented on Compute Engine. You want to increase the durability of your application. What should you do?;Perform health checks on your Compute Engine instances.;Implement a scheduled snapshot on your Compute Engine instances.;Monitor your application’s usage metrics and implement autoscaling.;Implement a regional managed instance group.;d;According to Google Cloud Architecture Framework, a Regional Managed Instance Group (MIG)  increases application durability (resilience) by distributing instances across multiple zones, allowing the application to survive a zonal outage.;medium;EN
Cloud Build;Developers on your team frequently write new versions of the code for one of your applications. You want to automate the build process when updates are pushed to Cloud Source Repositories. What should you do?;Implement a build trigger that references your repository and branch.;Set proper permissions for Cloud Build to access deployment resources.;Implement a Cloud Build configuration file with build steps.;Upload application updates and Cloud Build configuration files to Cloud Source Repositories.;a;According to Cloud Build documentation, to automate builds in response to source code changes (like a push to a specific branch), you must create a Build Trigger that watches the specified repository and branch. ;easy;EN

DevOps;You need to adopt Site Reliability Engineering principles and increase visibility into your environment. You want to minimize management overhead and reduce noise generated by the information being collected. You also want to streamline the process of reacting to analyzing and improving your environment, and to ensure that only trusted container images are deployed to production. What should you do?;Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Only page the on-call contact about novel issues or events that haven’t been seen before. Use GNU Privacy Guard (GPG) to check container image signatures and ensure that only signed containers are deployed.;Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Only page the on-call contact about novel issues that violate an SLO or events that haven’t been seen before. Use Binary Authorization to ensure that only signed container images are deployed.;Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Page the on-call contact when issues that affect resources in the environment are detected. Use GPG to check container image signatures and ensure that only signed containers are deployed.;Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Page the on-call contact when issues that affect resources in the environment are detected. Use Binary Authorization to ensure that only signed container images are deployed.;b;According to Site Reliability Engineering (SRE) principles, to reduce noise (alert fatigue), you should only page on symptoms that affect the user, specifically **SLO (Service Level Objective) violations** , rather than all resource issues. Furthermore, **Binary Authorization**  is the managed GCP service designed to enforce deployment security policies (trusted containers) with minimal operational overhead compared to manual GPG verification.;hard;EN
Google Cloud Observability;Cymbal Direct’s warehouse and inventory system was written in Java. The system uses a microservices architecture in GKE and is instrumented with Zipkin. Seemingly at random, a request will be 5-10 times slower than others. The development team tried to reproduce the problem in testing, but failed to determine the cause of the issue. What should you do?;Create metrics in Cloud Monitoring for your microservices to test whether they are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Profiler to determine which functions/methods in your application’s code use the most system resources. Use Cloud Trace to identify slow requests and determine which microservices/calls take the most time to respond.;Use Error Reporting to test whether your microservices are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Trace to determine which functions/methods in your application’s code Use the most system resources. Use Cloud Profiler to identify slow requests and determine which microservices/calls take the most time to respond.;Create metrics in Cloud Monitoring for your microservices to test whether they are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Trace to determine which functions/methods in your application’s code use the most system resources. Use Cloud Profiler to identify slow requests and determine which microservices/calls take the most time to respond.;Use Error Reporting to test whether your microservices are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Profiler to determine which functions/methods in your application’s code use the most system resources. Use Cloud Trace to identify slow requests and determine which microservices/calls take the most time to respond.;a;According to Google Cloud Observability documentation, the correct tool mapping is: **Cloud Trace** for analyzing latency/slowness in distributed microservices (supporting Zipkin) , **Cloud Profiler** for identifying code-level resource consumption (heavy functions) , and **Cloud Monitoring** for tracking availability and latency metrics.;medium;EN
Cloud Billing & IAM;Cymbal Direct is working on a social media integration service in Google Cloud. Mahesh is a non-technical manager who wants to ensure that the project doesn’t exceed the budget and responds quickly to unexpected cost increases. You need to set up access and billing for the project. What should you do?;Use the predefined Billing Account Administrator role for the Billing Administrator group, and assign Mahesh to the group. Create a project budget. Configure billing alerts to be sent to the Billing Administrator. Use resource quotas to cap how many resources can be deployed.;Assign the predefined Billing Account Administrator role to Mahesh. Create a project budget. Configure billing alerts to be sent to the Project Owner. Use resource quotas to cap how much money can be spent.;Use the predefined Billing Account Administrator role for the Billing Administrator group, and assign Mahesh to the group. Create a project budget. Configure billing alerts to be sent to the Billing Account Administrator. Use resource quotas to cap how much money can be spent.;Assign the predefined Billing Account Administrator role to Mahesh. Create a project budget. Configure billing alerts to be sent to the Billing Administrator. Use resource quotas to cap how many resources can be deployed.;a;According to Google Cloud IAM best practices, roles should be assigned to groups rather than individuals to ease management. Furthermore, Resource Quotas  are technically designed to limit the *quantity* of deployable resources (e.g., CPU cores), preventing runaway usage, whereas they do not directly cap monetary spend (which is monitored via Budgets).;medium;EN
Cloud Monitoring;Your client has adopted a multi-cloud strategy that uses a virtual machine-based infrastructure. The client's website serves users across the globe. The client needs a single dashboard view to monitor performance in their AWS and Google Cloud environments. Your client previously experienced an extended outage and wants to establish a monthly service level objective (SLO) of no outage longer than an hour. What should you do?;In Cloud Monitoring, create an uptime check for the URL your clients will access. Configure it to check from multiple regions. Use the Cloud Monitoring dashboard to view the uptime metrics over time and ensure that the SLO is met. Recommend an SLO of 97% uptime per month.;Create a new project to use as an AWS connector project. Authorize access to the project from AWS with a service account. Install the monitoring agent on AWS EC2 (virtual machines) and Compute Engine instances. Use Cloud Monitoring to create dashboards that use the performance metrics from virtual machines to ensure that the SLO is met.;Authorize access to your Google Cloud project from AWS with a service account. Install the monitoring agent on AWS EC2 (virtual machines) and Compute Engine instances. Use Cloud Monitoring to create dashboards that use the performance metrics from virtual machines to ensure that the SLO is met.;In Cloud Monitoring, create an uptime check for the URL your clients will access. Configure it to check from multiple regions. Use the Cloud Monitoring dashboard to view the uptime metrics over time and ensure that the SLO is met. Recommend an SLO of 97% uptime per day.;d;According to SRE principles, SLOs should be measured from the user's perspective (Uptime Checks) rather than internal metrics (Agents). Furthermore, a monthly SLO of 97% allows for ~21 hours of downtime, which fails the requirement of "no outage longer than an hour," whereas a **Daily** SLO of 97% (allowing ~43 minutes of downtime/day) strictly enforces the client's constraint.;hard;EN
Cloud Monitoring;Cymbal Direct uses a proprietary service to manage on-call rotation and alerting. The on-call rotation service has an API for integration. Cymbal Direct wants to monitor its environment for service availability and ensure that the correct person is notified. What should you do?;Ensure that VPC firewall rules allow access from the IP addresses used by Google Cloud's uptime-check servers. Create a Pub/Sub topic for alerting as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's external IP address, with an alerting policy set to use the Pub/Sub topic. Create a Cloud Run function that subscribes to the Pub/Sub topic to send the alert to the on-call API.;Ensure that VPC firewall rules allow access from the on-call API. Create a Cloud Run function to send the alert to the on-call API. Add Cloud Run functions as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's external IP address, with an alerting policy set to use the Cloud Run function.;Ensure that VPC firewall rules allow access from the IP addresses used by Google Cloud's uptime-check servers. Add the URL for the on-call rotation API as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's internal IP address, with an alerting policy set to use the API.;Ensure that VPC firewall rules allow access from the IP addresses used by Google Cloud’s uptime-check servers. Create a Pub/Sub topic for alerting as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's internal IP address, with an alerting policy set to use the Pub/Sub topic. Create a Cloud Run function that subscribes to the Pub/Sub topic to send the alert to the on-call API.;a;Standard Google Cloud Uptime Checks originate from public IP ranges  and target External IPs, requiring specific firewall allow rules. To integrate with a custom/proprietary API, the recommended architectural pattern is to send the alert to a **Pub/Sub topic**, which triggers a **Cloud Run function** to format the message and call the API.;medium;EN
DevOps;Cymbal Direct releases new versions of its drone delivery software every 1.5 to 2 months. Although most releases are successful, you have experienced three problematic releases that made drone delivery unavailable while software developers rolled back the release. You want to increase the reliability of software releases and prevent similar problems in the future. What should you do?;Adopt a “waterfall” development process. Maintain the current release schedule. Ensure that documentation explains how all the features interact. Ensure that the entire application is tested in a staging environment before the release. Ensure that the process to roll back the release is documented. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility.;Adopt an “agile” development process. Reduce the time between releases as much as possible. Automate the build process from a source repository, which includes versioning and self-testing. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility. Use a canary deployment to detect issues that could cause rollback.;Adopt a “waterfall” development process. Maintain the current release schedule. Ensure that documentation explains how all the features interact. Automate testing of the application. Ensure that the process to roll back the release is well documented. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility.;Adopt an “agile” development process. Maintain the current release schedule. Automate build processes from a source repository. Automate testing after the build process. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility. Deploy the previous version if problems are detected and you need to roll back.;b;According to Google Cloud DevOps and SRE best practices, reducing the batch size of releases (frequent, small updates via Agile) reduces the risk of failure. Furthermore, implementing **Canary deployments**  allows you to test updates on a small subset of users first, preventing system-wide downtime and eliminating the need for complex, full-fleet rollbacks if issues arise.;medium;EN
Cloud Billing & Cost Management;Your environment has multiple projects used for development and testing. Each project has a budget, and each developer has a budget. A personal budget overrun can cause a project budget overrun. Several developers are creating resources for testing as part of their CI/CD pipeline but are not deleting these resources after their tests are complete. If the compute resource fails during testing, the test can be run again. You want to reduce costs and notify the developer when a personal budget overrun causes a project budget overrun. What should you do?;Configure billing export to BigQuery. Create a Google Cloud budget for each project. Configure a billing alert to notify billing admins and users when their budget is exceeded. Modify the build scripts/pipeline to label all resources with the label “creator” set to the developer’s email address. Use spot (preemptible) instances wherever possible.;Configure billing export to BigQuery. Create a Google Cloud budget for each project. Create a Pub/Sub topic for developer-budget-notifications. Create a Cloud Run function to notify the developer based on the labels. Modify the build scripts/pipeline to label all resources with the label “creator” set to the developer’s email address. Use spot (preemptible) instances wherever possible.;Configure billing export to BigQuery. Create a Google Cloud budget for each project. Create a Pub/Sub topic for developer-budget-notifications. Create a Cloud Run function to notify the developer based on the labels. Modify the build scripts/pipeline to label all resources with the label “creator” set to the developer’s email address. Use spot (preemptible) instances wherever possible. Use Cloud Scheduler to delete resources older than 24 hours in each project.;Configure billing export to BigQuery. Create a Google Cloud budget for each project. Create a group for the developers in each project, and add them to the appropriate group. Create a notification channel for each group. Configure a billing alert to notify the group when their budget is exceeded. Modify the build scripts/pipeline to label all resources with the label “creator” set to the developer’s email address. Use spot (preemptible) instances wherever possible.;b;According to GCP Cost Management documentation, to send dynamic notifications to specific users (the "creator") based on granular cost attribution, you must use **Programmatic Budget Notifications**. This involves connecting a Budget to a **Pub/Sub** topic, which triggers a **Cloud Run function**. The function can then query the **Billing Export** (BigQuery) to analyze usage by the `creator` label and identify who exceeded their personal limit, satisfying the requirement to notify the specific developer. Standard alerts (Option A/D) cannot route to dynamic labels.;hard;EN
Cloud Architecture;Your organization is planning a disaster recovery (DR) strategy. Your stakeholders require a recovery time objective (RTO) of 0 and a recovery point objective (RPO) of 0 for zone outage. They require an RTO of 4 hours and an RPO of 1 hour for a regional outage. Your application consists of a web application and a backend MySQL database. You need the most efficient solution to meet your recovery KPIs. What should you do?;Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to use both backends. Use Cloud SQL with high availability (HA) enabled in us-east and back up the database every hour to a multi-region Cloud Storage bucket. Restore the data to a Cloud SQL database in us-west if there is a failure.;Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to use both backends. Use Cloud SQL with high availability (HA) enabled in us-east and a cross-region replica in us-west.;Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to the us-east backend. Use Cloud SQL with high availability (HA) enabled in us-east and a cross-region replica in us-west. Manually promote the us-west Cloud SQL instance and change the load balancer backend to us-west.;Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to use both backends. Use Cloud SQL with high availability (HA) enabled in us-east and back up the database every hour to a multi-region Cloud Storage bucket. Restore the data to a Cloud SQL database in us-west if there is a failure and change the load balancer backend to us-west.;c;To meet the Regional RPO of 1 hour and RTO of 4 hours without the risks of backup delays, a **Cross-Region Read Replica**  is required (providing near-zero RPO). The Load Balancer should be configured Active-Passive (pointing only to `us-east` initially) to prevent traffic from hitting the secondary region where the database is read-only until a manual promotion and failover occur;hard;EN
DevOps;Cymbal Direct has a new social media integration service that pulls images of its products from social media sites and displays them in a gallery of customer images on your online store. You receive an alert from Cloud Monitoring at 3:34 AM on Saturday. The store is still online, but the gallery does not appear. The CPU utilization is 30% higher than expected on the VMs running the service, which causes the managed instance group (MIG) to scale to the maximum number of instances. You verify that the issue is real by checking the site, and verify that it is not CPU-related by checking the incidents timeline. What should you do to resolve the issue?;Increase the maximum number of instances in the MIG and verify that this resolves the issue.;Increase the maximum number of instances in the MIG and verify that this resolves the issue. Check the incident documentation or labels to determine the on-call contact. Appoint an incident commander, and open a chat channel, or conference call for emergency response. Investigate and resolve the root cause of the issue. Write a blameless post-mortem and identify steps to prevent the issue, to ensure a culture of continuous improvement.;Increase the maximum number of instances in the MIG and verify that this resolves the issue. Ensure that the ticket is annotated with your solution. Create a normal work ticket for the application developer with a link to the incident. Mark the incident as closed.;Check the incident documentation or labels to determine the on-call contact. Appoint an incident commander, and open a chat channel, or conference call for emergency response. Investigate and resolve the issue by increasing the maximum number of instances in the MIG, and verify that this resolves the issue. Mark the incident as closed.;a;Cloud Monitoring alerting policies are state-based the incident is automatically closed by the system once the triggering condition (high CPU) is no longer met. Therefore, the correct action is simply to mitigate the capacity issue (Increase MIG), which lowers the CPU utilization and allows the system to auto-resolve the incident, without requiring manual closure or a high-severity response (Incident Commander) for a non-critical feature.;hard;EN
########################
###### CORE INFRA ######
########################
Regions and Zones;What is the primary benefit to a Google Cloud customer of using resources in several zones within a region?;For expanding services to customers in new areas;For better performance;For getting discounts on other zones;For improved fault tolerance;d;According to Google Cloud documentation, zones are designed as independent failure domains (with separate power, cooling, and networking) within a region. Therefore, the primary benefit of distributing resources across multiple zones is improved fault tolerance and high availability against single-zone failures.;easy;EN
Cloud Computing Concepts;What type of cloud computing service lets you bind your application code to libraries that give access to the infrastructure your application needs?;Infrastructure as a service;Software as a service;Hybrid cloud;Platform as a service;d;According to Google Cloud's "Core Infrastructure" course definitions, Platform as a Service (PaaS) is the model where you bind application code to libraries that provide access to the necessary infrastructure, abstracting away the underlying hardware management.;easy;EN
Regions and Zones;Why might a Google Cloud customer use resources in several regions around the world?;To earn discounts;To offer localized application versions in different regions.;To improve security;To bring their applications closer to users around the world, and for improved fault tolerance;d;According to Google Cloud's "Geography and Regions" documentation, using multiple regions allows you to place applications closer to global users to reduce latency (improve performance) and ensures higher fault tolerance and disaster recovery by isolating resources against region-wide outages.;easy;EN

Resource Hierarchy;Choose the correct completion: Services and APIs are enabled on a per-__________ basis.;Folder;Project;Billing account;Organization;b;According to Google Cloud documentation, projects are the base-level organizing entity where you enable and manage APIs and services. While projects belong to organizations and folders, the actual activation of a service happens at the project level.;easy;EN
Resource Hierarchy;Which of these values is globally unique, permanent, and unchangeable, but can be modified by the customer during creation?;The project's billing credit-card number;The project name;The project ID;The project number;c;According to Google Cloud documentation, the Project ID is a globally unique and immutable identifier that cannot be changed after creation, but users can explicitly choose or modify the Project ID string during the initial project creation process (unlike the Project Number, which is auto-assigned).;easy;EN
IAM;Order these IAM role types from broadest to finest-grained.;Custom roles, predefined roles, basic roles;Basic roles, predefined roles, custom roles;Predefined roles, custom roles, basic roles;;b;According to Google Cloud IAM documentation, Basic (Primitive) roles are the broadest as they affect resources project-wide. Predefined roles grant granular access to specific services, and Custom roles are the finest-grained as they allow the selection of specific permissions.;easy;EN

Cloud Load Balancing;How does global Cloud Load Balancing allow you to balance HTTP-based traffic?;Across multiple Google Cloud services.;Across multiple virtual machine instances in a single Compute Engine region.;Across multiple physical machines in a single data center.;Across multiple Compute Engine regions.;d;According to Google Cloud documentation, Global HTTP(S) Load Balancing is designed to distribute traffic across backend instances located in multiple Compute Engine regions using a single global IP address, allowing for cross-region failover and proximity-based routing.;medium;EN
VPC Network;Which of the following describes a Virtual Private Cloud (VPC)?;A secure, individual, private cloud-computing model hosted within a public cloud.;An option that allows one or more direct, private connections to Google.;A fully distributed, software-defined, managed service for traffic distribution.;A service that routes traffic through a Google Point of Presence (PoP).;a;According to Google Cloud documentation, a VPC is a virtual version of a physical network implemented inside of Google's public cloud, providing a secure, logically isolated private network for your cloud resources (essentially a private cloud within a public cloud).;easy;EN
Hybrid Connectivity;For which of these interconnect options is a Service Level Agreement available?;Standard Network Tier;Dedicated Interconnect;Direct Peering;Carrier Peering;b;According to Google Cloud documentation, Dedicated Interconnect includes a Service Level Agreement (SLA) (guaranteeing 99.9% or 99.99% uptime depending on topology), whereas Direct Peering and Carrier Peering are explicitly listed as having no SLA.;medium;EN
Virtual Private Cloud (VPC);In Google Cloud VPCs, what scope do subnets have?;Regional;Zonal;Global;Multi-regional;a;According to Google Cloud documentation, VPC networks are global resources, but subnets are regional resources. A subnet defines a range of IP addresses within a specific region and spans all zones in that region.;easy;EN

Cloud Storage;What is the correct use case for Cloud Storage?;Cloud Storage is well suited to providing the root file system of a Linux virtual machine.;Cloud Storage is well suited to providing durable and highly available object storage.;Cloud Storage is well suited to providing data warehousing services.;Cloud Storage is well suited to providing RDBMS services.;b;Cloud Storage is Google Cloud's object storage service designed for storing unstructured data (objects) with high durability and availability. It is not suitable for VM boot disks (which use Persistent Disk), data warehousing (which uses BigQuery), or RDBMS (which uses Cloud SQL).;easy;EN
Databases;Which relational database service can scale to higher database sizes?;Spanner;Cloud SQL;Bigtable;Firestore;a;Cloud Spanner is a fully managed relational database service that offers unlimited horizontal scaling , unlike Cloud SQL which relies on vertical scaling. Bigtable and Firestore are NoSQL databases.;medium;EN
Cloud Storage;Why would a customer consider the Coldline storage class?;To save money on storing infrequently accessed data.;To save money on storing frequently accessed data.;To improve security.;To use the Coldline Storage API.;a;According to Google Cloud documentation, Coldline Storage is designed for data accessed infrequently (at most once a quarter). It offers lower storage costs than Standard or Nearline storage, making it cost-effective for long-term retention of data that is rarely read.;easy;EN

Google Kubernetes Engine;Where do the resources used to build Google Kubernetes Engine clusters come from?;Cloud Storage;App Engine;Compute Engine;Bare-metal servers;c;According to Google Cloud documentation, standard GKE clusters consist of a control plane and worker nodes, where the nodes are provisioned as Compute Engine virtual machine instances that run your containerized applications.;easy;EN
Kubernetes;What is a Kubernetes pod?;A group of nodes;A group of clusters;A group of VMs;A group of containers;d;According to Kubernetes and GKE documentation, a Pod is the smallest deployable unit of computing that consists of one or more containers sharing storage and network resources.;easy;EN

Serverless Computing;Why might a Google Cloud customer choose to use Cloud Run functions?;Cloud Run functions is the primary way to run C++ applications in Google Cloud.;Their application contains event-driven code that they don't want to provision compute resources for.;Cloud Run functions is a free service for hosting compute operations.;Their application has a legacy monolithic structure that they want to separate into microservices.;b;Cloud Run functions (formerly Cloud Functions) is a serverless, FaaS (Function as a Service) product specifically designed for single-purpose, event-driven code (like processing file uploads or Pub/Sub messages) without the need to provision or manage underlying compute infrastructure.;easy;EN
Cloud Run functions;Which of these statements about Cloud Run functions are correct? Select three: (1) Is a scalable functions-as-a-service platform; (2) Require servers or VMs to be provisioned; (3) Can only be invoked by sending HTTP requests; (4) Is integrated with Cloud Logging; (5) Can be used to extend Cloud services.;1, 2, and 3;1, 4, and 5;2, 3, and 4;3, 4, and 5;b;According to Google Cloud documentation, Cloud Run functions (formerly Cloud Functions) is a serverless, scalable FaaS platform (1) that integrates with Cloud Logging (4) and is often used to connect and extend cloud services (5). It does not require provisioning servers (2 is false) and supports event-driven triggers via Eventarc/PubSub in addition to HTTP (3 is false).;easy;EN
Cloud Run functions;Why might a Google Cloud customer choose to use Cloud Run functions?;Their application contains event-driven code that they don't want to provision compute resources for.;Cloud Run functions is a free service for hosting compute operations.;Cloud Run functions is the primary way to run C++ applications in Google Cloud.;Their application has a legacy monolithic structure that they want to separate into microservices.;a;According to Google Cloud documentation, Cloud Run functions (formerly Cloud Functions) is a serverless, event-driven FaaS platform. It allows developers to run single-purpose code in response to events without the need to provision or manage underlying compute infrastructure (servers).;easy;EN

########################
#### CORE SERVICES #####
########################



